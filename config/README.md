# Config File Documentation

**Please note that the suggested settings in the configuration file provided is not fine-tuned**, one has to adjust the settings based on the data. The description of parameters in the configuration file:

## Model settings

| Parameter     | Description                                                                                                                 | Note                                                               |
|---------------|-----------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|
| model          | The name of the neural network models for training.                                                                                 | Currently, the code supports the DNN, RNNs (i.e., LSTM and GRU ), BiRNN (i.e., bidirectional LSTM and bidirectional GRU), and textCNN)                                          |
| optimizer | The optimizer used.                                                                                                | A user can choose different optimizers based on their tasks. In this code, we use the SGD with its default settings.                                                                     |
| loss_function    | The loss function to minimize. | We use the binary cross entropy. |
| handle_data_imbalance    | Whether to handle the data imbalance issue. | The cost-sensitive learning will be applied if it is set to True|
| max_sequence_length   | The length for each input code sequence. | In this code, we use 1,000 as the maximal input length.|
| embedding_dim   | The dimensionality of the word vectors. | In this code, we use 100. After embedding, each input sequence will be a tensor with the shape of (1000, 100).  |
| use_dropout   | Whether to use dropout | In this code, we use a dropout to prevent overfitting. |
| dropout_rate   | The dropout value. | In this code, we use the value of 0.5.|
| dnn_size   | The number of neurons used for DNN (the first layer) | In this code, we set this value to 128.|
| rnn_size   | The number of neurons used for RNNs (the first layer) | In this code, we set this value to 128.|
| birnn_size   | The number of neurons used for bidirectional RNNs (the first layer) | In this code, we set this value to 64.

## Training settings

| Parameter     | Description                                                                                                                 | Note                                                               |
|---------------|-----------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|
| Test_set_ratio   | If not using a separate test set, set the test set ratio.  | In this code, we partition the dataset into training, validation and test sets with a ratio of 6:2:2. Users can use their own test set. If users use their own test set, they should set the 'using_separate_test_set' to True and ignore this value. |
| using_separate_test_set   | Suggest whether use a separate test set. | If this is set to True, please specify the path of test set.  |
| test_set_path   | The path contains the test data.  | If a user uses a separate test set, the user should specify a path leading to the test data. |
| Validation_set_ratio   | Suggest the percentage of the total dataset that is used as the validation set. | We use part of the training set as the validation set. |
| batch_size   | The size of mini batch | A relatively small batch size leads to better generalization. We use the value of 16. |
| epochs   | The number of forward and backward pass of all the training examples. | In this code, we set this value to 150.|
| patcience   | The number of epochs with no improvement after which training will be stopped. | In this code, we set this value to 35.|
| save_training_history   | Whether to save the training history. | The default value is True.|
| plot_training_history   | Whether to plot the training/validation curve. | The default value is True.|
| validation_metric   | The quantity to be monitored. | In this code, we choose to monitor validation loss.|
| tokenizer_path   | The path where the tokenized information is stored. | The tokenized information is stored in a Pickle file generated by the ```Word_to_vec_embedding.py``` script.|
| embedding_model_path   | The path where the trained Word2vec model is stored. | The trained Word2vec model (a dictionary) is a TXT file generated by the ```Word_to_vec_embedding.py``` script. |
| save_best_model   | If save_best_only=True, the latest best model according to the quantity monitored will not be overwritten | In this code, we set this value to True.|
| period_of_saving   | Interval (number of epochs) between checkpoints. | In this code, we set this value to 1.|
| log_path   | The path where the log files are stored. | By default, the log files are stored in the `logs/`.|
| model_save_path   | The path where the trained models are stored. | By default, the trained models are stored in the `result/models/`.|
| model_saved_name   | The name of the trained model. | By default, the trained model is called 'test_model'.
